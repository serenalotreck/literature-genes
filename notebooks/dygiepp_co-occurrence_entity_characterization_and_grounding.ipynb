{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyGIE++ co-occurrence entity characterization and grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import taxoniq\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb2hex\n",
    "import json\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_graphml('../data/kg/all_drought_dt_co_occurrence_graph_02May2024.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_by_type = defaultdict(list)\n",
    "for n, attrs in graph.nodes(data=True):\n",
    "    ents_by_type[attrs['ent_type']].append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check database grounding potential for `Multi`- and `Unicellular_organism` types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounded_multicell = {}\n",
    "for n in ents_by_type['Multicellular_organism']:\n",
    "    try:\n",
    "        t = taxoniq.Taxon(scientific_name=n)\n",
    "        grounded_multicell[n] = t.scientific_name\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(f'{len(grounded_multicell)} of {len(ents_by_type[\"Multicellular_organism\"])} multicellular organisms could be grounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounded_unicell = {}\n",
    "for n in ents_by_type['Unicellular_organism']:\n",
    "    try:\n",
    "        t = taxoniq.Taxon(scientific_name=n)\n",
    "        grounded_unicell[n] = t.scientific_name\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(f'{len(grounded_unicell)} of {len(ents_by_type[\"Unicellular_organism\"])} unicellular organisms could be grounded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grounding capacity directly using taxoniq is horrendous. What happens if we concatenate all of the species mentions together and use TaxoNERD, like we did for citation network classification? We're going to write a mini script and submit this as a job, because TaxoNERD grounding is exceedingly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/kg/full_graph_multicellular_ents_02May2024.txt', 'w') as f:\n",
    "#     f.write(\"\\n\".join(ents_by_type['Multicellular_organism']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/kg/full_graph_multicellular_ents_GROUNDED_02May2024.json') as f:\n",
    "    grounded_ents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(grounded_ents)} that received a grounding, and {len(set(grounded_ents.values()))} unique groundings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great news! It means that a bunch of entities got resolved, even though we didn't get groundings for many of the entities. Let's use this to tighten up our graph. Unfortunately using the existing networkx function `contracted_nodes` is prohibitively slow on large graphs, so let's try something based on [this solution](https://stackoverflow.com/a/73762332/13340814) from stack overflow. In the stack overflow post, there are garanteed to be edges between the nodes to be contracted, whereas there is not in our case, so we'll have to improvise a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundings_to_ents = defaultdict(list)\n",
    "for ent, grd in grounded_ents.items():\n",
    "    groundings_to_ents[grd].append(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_groups(graph, groups):\n",
    "    \"\"\"\n",
    "    Contract groups of nodes that may or may not be connected. Combines the number\n",
    "    of per-doc mentions for edges and nodes, and keeps the oldest year as the first_year_mentioned.\n",
    "    \n",
    "    graph, newtorkx Graph: undirected network containing nodes in group\n",
    "    groups, dict of list: nodes to coalesce.\n",
    "    \"\"\"\n",
    "    # Convert original graph to node and edgelist\n",
    "    nodes = graph.nodes(data=True)\n",
    "    edges = nx.to_pandas_edgelist(graph)\n",
    "    \n",
    "    # Make sure edge years are integers for later use\n",
    "    edges = edges.astype({'first_year_mentioned': 'int32'})\n",
    "    \n",
    "    # Go through the groups\n",
    "    nodes_to_add = []\n",
    "    nodes_to_remove = []\n",
    "    for grounding, n_list in tqdm(groups.items()):\n",
    "        \n",
    "        # Get the oldest year for node mentions\n",
    "        oldest_node_year = min([int(nodes[n]['first_year_mentioned']) for n in n_list])\n",
    "        \n",
    "        # Get total node mentions\n",
    "        total_node_mentions = sum([nodes[n]['num_doc_mentions_all_time'] for n in n_list])\n",
    "        \n",
    "        # Get all uids of origin\n",
    "        combined_node_uids = ', '.join([nodes[n]['uids_of_origin'] for n in n_list])\n",
    "        \n",
    "        # Get the formal name that we want to keep for the node\n",
    "        try:\n",
    "            formal_name = taxoniq.Taxon(int(grounding)).scientific_name\n",
    "        except KeyError:\n",
    "            formal_name = groups[grounding][0]\n",
    "        \n",
    "        # Get the subset of relations that involve these nodes\n",
    "        edge_subset = edges[(edges['source'].isin(n_list)) | (edges['target'].isin(n_list))].reset_index(drop=True)\n",
    "        \n",
    "        # Replace all nodes with the formal representation to being coalescing\n",
    "        edges_replaced = edge_subset.replace(to_replace=n_list, value=formal_name)\n",
    "        \n",
    "        # Combine the values of any edges that are semantically identical after replacement\n",
    "        # First, get the indices of repeated groups, order-agnostically\n",
    "        tup_list = [tuple(set(tup)) for tup in list(edges_replaced[['source', 'target']].itertuples(index=False, name=None)) if len(set(tup)) > 1]\n",
    "        tup_set = set(tup_list)\n",
    "        rep_idxs = defaultdict(list)\n",
    "        for i, tup in enumerate(tup_list):\n",
    "            rep_idxs[tup].append(i)\n",
    "        # Now, combine the attributes and store in a dict\n",
    "        edge_replacements = []\n",
    "        keep_the_same = []\n",
    "        for edge, idxs in rep_idxs.items():\n",
    "            if len(idxs) > 1:\n",
    "                oldest_edge_year = edges_replaced.loc[idxs, 'first_year_mentioned'].min()\n",
    "                total_edge_mentions = edges_replaced.loc[idxs, 'num_doc_mentions_all_time'].sum()\n",
    "                is_drought = edges_replaced.loc[idxs, 'is_drought'].any()\n",
    "                is_desiccation = edges_replaced.loc[idxs, 'is_desiccation'].any()\n",
    "                uids_of_origin = ', '.join(edges_replaced.loc[idxs, 'uids_of_origin'])\n",
    "                edge_replacements.append({\n",
    "                    'source': edge[0],\n",
    "                    'target': edge[1],\n",
    "                    'first_year_mentioned': oldest_edge_year,\n",
    "                    'num_doc_mentions_all_time': total_edge_mentions,\n",
    "                    'is_drought': is_drought,\n",
    "                    'is_desiccation': is_desiccation,\n",
    "                    'uids_of_origin': uids_of_origin\n",
    "                })\n",
    "            elif len(idxs) == 1:\n",
    "                keep_the_same.extend(idxs)\n",
    "        # Now drop all indices that had more than one semantic replicate\n",
    "        edges_replaced_to_drop = edge_subset.loc[~edges_replaced.index.isin(keep_the_same)]\n",
    "        edges = pd.merge(edges, edges_replaced_to_drop, how='outer', indicator=True)\n",
    "        edges = edges.loc[edges._merge == 'left_only'].drop(columns='_merge')\n",
    "        # And replace with the combined edges\n",
    "        edges = pd.concat([edges, pd.DataFrame(edge_replacements)], ignore_index=True)\n",
    "        \n",
    "        # And finally, save the formal name of the new node and its attrs to use later, and add the nodes to remove\n",
    "        nodes_to_add.append((formal_name,\n",
    "                             {'first_year_mentioned': oldest_node_year,\n",
    "                              'num_doc_mentions_all_time': total_node_mentions,\n",
    "                             'uids_of_origin': combined_node_uids,\n",
    "                             'entity_type': 'Multicellular_organism'})) # Since this was all we could ground\n",
    "        nodes_to_remove.extend(n_list)\n",
    "    \n",
    "    # Remove old nodes and add new ones\n",
    "    nodes_processed = [(n, attrs) for n, attrs in nodes if n not in nodes_to_remove]\n",
    "    for new_node in nodes_to_add:\n",
    "        nodes_processed.append(new_node)\n",
    "    \n",
    "    # Make new graph from edgelist and nodelist\n",
    "    new_graph = nx.from_pandas_edgelist(edges, edge_attr=['first_year_mentioned',\n",
    "                    'num_doc_mentions_all_time', 'is_drought', 'is_desiccation',\n",
    "                    'uids_of_origin'])\n",
    "    _ = new_graph.add_nodes_from(nodes_processed)\n",
    "    \n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a small subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_test = nx.to_pandas_edgelist(graph).loc[:5,:]\n",
    "sesame_test.loc[1, 'target'] = 'sesame seed'\n",
    "sesame_test.loc[5, 'source'] = 'sesame plant'\n",
    "sesame_test.loc[5, 'target'] = 'peg-induced drought tolerance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_graph = nx.from_pandas_edgelist(sesame_test, edge_attr=[\n",
    "    'is_drought',\n",
    "    'uids_of_origin',\n",
    "    'first_year_mentioned',\n",
    "    'num_doc_mentions_all_time',\n",
    "    'is_desiccation'\n",
    "])\n",
    "actual_nodes = [(n, attr) for n, attr in graph.nodes(data=True) if (n in sesame_test.source.tolist()) or (n in sesame_test.target.tolist())]\n",
    "_ = sesame_graph.add_nodes_from(actual_nodes + [('sesame seeds', {'uids_of_origin': 'WOS:000623658100043',\n",
    "                                                   'entity_type': 'Multicellular_organism',\n",
    "                                                  'first_year_mentioned': '2017',\n",
    "                                                  'num_doc_mentions_all_time': 3})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_pandas_edgelist(sesame_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_groups = {'4182': ['sesame', 'sesame seed', 'sesame plant', 'sesame seeds']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Correct number total mentions for combined entity:', sum([attrs['num_doc_mentions_all_time'] for n, attrs in sesame_graph.nodes(data=True) if n in sesame_groups['4182']]))\n",
    "print('Correct number of total mentions for the combined edge: 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = contract_groups(sesame_graph, sesame_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_pandas_edgelist(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_output.nodes(data=True)['Sesamum indicum']['first_year_mentioned'], test_output.nodes(data=True)['Sesamum indicum']['num_doc_mentions_all_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test speed against using the contraction function from networkx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraction(graph, groundings_to_ents):\n",
    "    for grd, nodes in tqdm(groundings_to_ents.items()):\n",
    "        first_node = nodes[0]\n",
    "        formal_name = taxoniq.Taxon(int(grd)).scientific_name\n",
    "        for n in nodes[1:]:\n",
    "            graph = nx.contracted_nodes(graph, first_node, n)\n",
    "        nx.relabel_nodes(graph, {first_node: formal_name})\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = timeit.Timer(partial(contraction, sesame_graph, sesame_groups))\n",
    "t1.timeit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = timeit.Timer(partial(contract_groups, sesame_graph, sesame_groups))\n",
    "t2.timeit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contracted_graph = contract_groups(graph, groundings_to_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contracted_graph = contraction(graph, groundings_to_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the networkx builtin in faster on this small subset, it does not at all scale with size. When I tried to run the `contraction` function above on my graph, it wouldn't even run the first iteration after several minutes, while my own code only took ~7 seconds per iteration. 7 seconds per iteration is still un-ideal, because it'll take 2 days to run on the entire graph; however, that is infintely preferable to the alternative. Therefore, going to make and submit a job to do the contraction, and will read in the results here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracted_graph = nx.read_graphml('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appearance of study organisms over time\n",
    "We'll get the top twenty species in the graph based on both the number of mentions of each node, as well as the number of different nodes with the same Taxonomy ID, and then look at when they appear over time. For the moment, we'll ignore any nodes that didn't get grounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO account for groundings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_mentions = nx.get_node_attributes(graph, 'num_doc_mentions_all_time')\n",
    "multi_mentions = {ent: ent_mentions[ent] for ent in ents_by_type['Multicellular_organism']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twenty_organisms = dict(sorted(multi_mentions.items(), key=lambda x:x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_year_mentions = {}\n",
    "for organism in top_twenty_organisms.keys():\n",
    "    year_mentions = {}\n",
    "    attrs = graph.nodes[organism]\n",
    "    for attr, val in attrs.items():\n",
    "        if ('num_mentions_' in attr) and (attr != 'num_doc_mentions_all_time'):\n",
    "            year = attr.split('_')[-1]\n",
    "            if year != '2023':\n",
    "                year_mentions[int(year)] = val\n",
    "    organism_year_mentions[organism] = year_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('tab20c')\n",
    "organism_colors = {organism: rgb2hex(cmap(i)) for i, organism in enumerate(top_twenty_organisms.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for org in top_twenty_organisms.keys():\n",
    "    x = sorted(organism_year_mentions[org].keys())\n",
    "    y = [organism_year_mentions[org][i] for i in x]\n",
    "    color = organism_colors[org]\n",
    "    plt.plot(x, y, color=color, marker='o', label=f'{org} ({top_twenty_organisms[org]} total mentions)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mentions')\n",
    "plt.legend(loc=(1.1,0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genes and proteins directly connected to an organism mention\n",
    "One way we can identify genes/proteins that belong to various species in our graph is to check whether they are directly connected to an organism mention. The original intention of the is-in relation was to perform this kind of linking. Since we couldn't use typed relations and instead had to rely on co-occurrence, we'll treat any link as a possible is-in link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_types = nx.get_node_attributes(graph, 'ent_type')\n",
    "genes_by_organism = {}\n",
    "proteins_by_organism = {}\n",
    "for n in ents_by_type['Multicellular_organism']:\n",
    "    neighbors = graph.neighbors(n)\n",
    "    genes = []\n",
    "    proteins = []\n",
    "    for m in neighbors:\n",
    "        if ent_types[m] in ['DNA', 'RNA']:\n",
    "            genes.append(m)\n",
    "        elif ent_types[m] == 'Protein':\n",
    "            proteins.append(m)\n",
    "    genes_by_organism[n] = genes\n",
    "    proteins_by_organism[n] = proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the oversimplification that any true Arabidopsis genes would start with `at`, then we can check what percentage of the connections are \"correct\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_arabidopsis = [g for g in genes_by_organism['arabidopsis'] if g[:2] == 'at']\n",
    "print(f'Assuming correct genes will start with At, {(len(correct_arabidopsis)/len(genes_by_organism[\"arabidopsis\"]))*100:.2f} percent of genes directly connected to Arabidopsis are correct.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(genes_by_organism[\"arabidopsis\"])} of {graph.degree(\"arabidopsis\")} edges ({(len(genes_by_organism[\"arabidopsis\"])/graph.degree(\"arabidopsis\"))*100:.2f}%) from the node \"arabidopsis\" are to genes.')\n",
    "print(f'{len(proteins_by_organism[\"arabidopsis\"])} of {graph.degree(\"arabidopsis\")} edges ({(len(proteins_by_organism[\"arabidopsis\"])/graph.degree(\"arabidopsis\"))*100:.2f}%) from the node \"arabidopsis\" are to proteins.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
