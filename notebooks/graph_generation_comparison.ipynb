{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph generation comparison\n",
    "In this notebook, we'll compare the outputs of a few different approaches to  graph building for characteristics like degree skew and entity grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/lotrecks/anaconda3/envs/ontogpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "from ontogpt.io.csv_wrapper import parse_yaml_predictions\n",
    "from os import listdir\n",
    "from os.path import splitext\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from random import sample\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = '../knowledge_graph/schema/desiccation.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 926/926 [00:00<00:00, 110004.40it/s]\n"
     ]
    }
   ],
   "source": [
    "onto_full_ents, onto_full_rels = parse_yaml_predictions('../data/ontogpt_output/test_1000_full/output.txt', schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929/929 [00:00<00:00, 112557.29it/s]\n"
     ]
    }
   ],
   "source": [
    "onto_slim_ents, onto_slim_rels = parse_yaml_predictions('../data/ontogpt_output/test_1000_slim/output.txt', schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_to_keep = [splitext(f)[0] for f in listdir('/mnt/scratch/lotrecks/drought_and_des_1000_subset_15Apr2024/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../data/dygiepp/model_predictions/09Apr24_dygiepp_test_formatted_data_pickle_predictions.jsonl') as reader:\n",
    "    dygiepp = [obj for obj in reader if obj['doc_key'] in uids_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n",
    "Convert DyGIE++ to csv, then all three to networkx objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dygiepp_ents = {'id': [], 'category': [], 'name': [], 'provided_by': []}\n",
    "dygiepp_rels = {'id': [], 'category': [], 'provided_by': [], 'predicate': [], 'subject': [], 'object': []}\n",
    "for doc in dygiepp:\n",
    "    all_text = [tok for sent in doc['sentences'] for tok in sent]\n",
    "    for sent in doc['predicted_ner']:\n",
    "        for ent in sent:\n",
    "            dygiepp_ents['id'].append(np.nan)\n",
    "            dygiepp_ents['category'].append(ent[2])\n",
    "            dygiepp_ents['name'].append(' '.join(all_text[ent[0]: ent[1]+1]).lower())\n",
    "            dygiepp_ents['provided_by'].append(doc['doc_key'])\n",
    "    for sent in doc['predicted_relations']:\n",
    "        for rel in sent:\n",
    "            dygiepp_rels['id'].append(np.nan)\n",
    "            dygiepp_rels['category'].append(rel[4])\n",
    "            dygiepp_rels['provided_by'].append(doc['doc_key'])\n",
    "            dygiepp_rels['predicate'].append(rel[4])\n",
    "            dygiepp_rels['subject'].append(' '.join(all_text[rel[0]: rel[1]+1]).lower())\n",
    "            dygiepp_rels['object'].append(' '.join(all_text[rel[2]: rel[3]+1]).lower())\n",
    "dygiepp_ent_df = pd.DataFrame(dygiepp_ents)\n",
    "dygiepp_rel_df = pd.DataFrame(dygiepp_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgx_to_networkx(ent_df, rel_df, source='onto'):\n",
    "    \"\"\"\n",
    "    Convert two KGX-formatted df's into a networkx graph.\n",
    "    \"\"\"\n",
    "    if source == 'onto':\n",
    "        nodes = [(row.id, {'ent_type': row.category, 'name': row['name']}) for i, row in ent_df.iterrows()]\n",
    "    else:\n",
    "        nodes = [(row.name, {'ent_type': row.category}) for i, row in ent_df.iterrows()]\n",
    "    edges = [(row.subject, row.object, {'rel_type': row.predicate}) for i, row in rel_df.iterrows()]\n",
    "    graph = nx.DiGraph()\n",
    "    _ = graph.add_nodes_from(nodes)\n",
    "    _ = graph.add_edges_from(edges)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dygiepp_graph = kgx_to_networkx(dygiepp_ent_df, dygiepp_rel_df, source='dygiepp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_slim_graph = kgx_to_networkx(onto_slim_ents, onto_slim_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_full_graph = kgx_to_networkx(onto_full_ents, onto_full_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = {\n",
    "    'dygiepp': dygiepp_graph,\n",
    "    'onto_slim': onto_slim_graph,\n",
    "    'onto_full': onto_full_graph\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic network statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In network dygiepp, there are 18360 nodes and 1448 edges, with a ratio of 0.07886710239651416.\n",
      "In network onto_slim, there are 2940 nodes and 770 edges, with a ratio of 0.2619047619047619.\n",
      "In network onto_full, there are 2893 nodes and 692 edges, with a ratio of 0.23919806429312132.\n"
     ]
    }
   ],
   "source": [
    "for net_name, net in nets.items():\n",
    "    print(f'In network {net_name}, there are {len(net.nodes)} nodes and {len(net.edges)} edges, with a ratio of {len(net.edges)/len(net.nodes)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of edges is bad in all methods, but OntoGPT extracts orders of magnitude more entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check relation quality\n",
    "A previous glance at the DyGIE++ relations showed that the majority of them were trivial -- we'll take a look at a few sets of triples from each graph to get an idea of whether or not they're meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten random triples from dygiepp:\n",
      "----------------------------------------------------\n",
      "('lq1230', 'activates', 'iaa')\n",
      "('salt tolerance', 'is-in', 'arabidopsis and grape callus')\n",
      "('gibberellins', 'activates', 'ramie growth')\n",
      "('gmnced3s', 'is-in', 'soybean')\n",
      "('vvmapk9', 'interacts', 'reactive oxygen species')\n",
      "('dhn1a_s', 'inhibits', 'b. cinerea growth')\n",
      "('aba', 'activates', \"tapp2abb '' -gamma\")\n",
      "('dhn1a_s', 'inhibits', 'lactate dehydrogenase activity')\n",
      "('dof family factors', 'is-in', 'chinese cabbage')\n",
      "('abscisic-acid', 'is-in', 'mosses')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ten random triples from onto_slim:\n",
      "----------------------------------------------------\n",
      "('NtNAK', 'ProteinOrganismRelationship', 'tobacco')\n",
      "('NF-YC', 'ProteinOrganismRelationship', 'Arabidopsis thaliana')\n",
      "('IVR1.2-3B', 'GeneOrganismRelationship', 'wheat (Triticum aestivum L.)')\n",
      "('AVP1', 'GeneProteinInteraction', 'H+-PPase')\n",
      "('YSK2-type dehydrin', 'ProteinOrganismRelationship', 'grape (Vitis vinifera)')\n",
      "('glutathione peroxidase', 'GeneProteinInteraction', 'transcription factor')\n",
      "('gamma-aminobutyric acid', 'GeneMoleculeInteraction', 'N/A')\n",
      "('MdZAT5', 'GeneOrganismRelationship', 'Arabidopsis')\n",
      "('dehydrin', 'ProteinMoleculeInteraction', 'proline')\n",
      "('None', 'GeneOrganismRelationship', 'Bromus inermis')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ten random triples from onto_full:\n",
      "----------------------------------------------------\n",
      "('maize', 'GeneOrganismRelationship', 'corn')\n",
      "('AtHSPR', 'ProteinOrganismRelationship', 'Arabidopsis thaliana')\n",
      "('ALDH proteins', 'ProteinOrganismRelationship', 'Gossypium raimondii')\n",
      "('ACC deaminase', 'ProteinMoleculeInteraction', 'ACC')\n",
      "('oslea3', 'GeneOrganismRelationship', 'Rice')\n",
      "('MPC1', 'GeneMoleculeInteraction', 'ABA')\n",
      "('FtsH-like protease', 'ProteinMoleculeInteraction', 'mRNA')\n",
      "('LEA', 'GeneProteinInteraction', 'unknown')\n",
      "('WRKY', 'ProteinMoleculeInteraction', 'ethylene')\n",
      "('H+-pyrophosphatase', 'ProteinMoleculeInteraction', 'NaCl')\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for net_name, net in nets.items():\n",
    "    trips = [(e1, attrs['rel_type'], e2) for e1, e2, attrs in net.edges(data=True)]\n",
    "    if net_name in ['onto_slim', 'onto_full']:\n",
    "        id2name = nx.get_node_attributes(net, 'name')\n",
    "        updated_trips = []\n",
    "        for e1, rt, e2 in trips:\n",
    "            try:\n",
    "                updated_trip = (id2name[e1], rt, id2name[e2])\n",
    "                updated_trips.append(updated_trip)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        trips = updated_trips\n",
    "    to_print = sample(trips, 10)\n",
    "    print(f'Ten random triples from {net_name}:')\n",
    "    print('----------------------------------------------------')\n",
    "    for trip in to_print:\n",
    "        print(trip)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These actually don't look bad at all! Certainly there are not enough of them, and there are a fair share of relations involving NaN or nonsense characters on the part of OntoGPT, but much less bad than I expected overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check grounding percentages and entity recovery for the slim and full OntoGPT versions\n",
    "I'm concerned that the slim version, while faster, results in substantially more entities not getting a grounding, so we want to check that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_groundings = onto_slim_ents[['name', 'id', 'category']].copy()\n",
    "slim_groundings['name'] = slim_groundings['name'].str.lower()\n",
    "slim_groundings = slim_groundings.drop_duplicates().reset_index(drop=True)\n",
    "full_groundings = onto_full_ents[['name', 'id', 'category']].copy()\n",
    "full_groundings['name'] = full_groundings['name'].str.lower()\n",
    "full_groundings = full_groundings.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's remove `AUTO` groundings if the same entity also has a true database grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abscisic acid</td>\n",
       "      <td>CHEBI:22152</td>\n",
       "      <td>Molecule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>norway maple</td>\n",
       "      <td>NCBITaxon:4025</td>\n",
       "      <td>Organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sycamore</td>\n",
       "      <td>AUTO:sycamore</td>\n",
       "      <td>Organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>AUTO:-</td>\n",
       "      <td>Gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acacia holosericea</td>\n",
       "      <td>NCBITaxon:1120455</td>\n",
       "      <td>Organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>gene</td>\n",
       "      <td>AUTO:gene</td>\n",
       "      <td>Gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>plants</td>\n",
       "      <td>AUTO:plants</td>\n",
       "      <td>Gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>brassica</td>\n",
       "      <td>NCBITaxon:3705</td>\n",
       "      <td>Organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>plant/algal macrofossils</td>\n",
       "      <td>AUTO:plant/algal%20macrofossils</td>\n",
       "      <td>Molecule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>quercus cerris</td>\n",
       "      <td>NCBITaxon:39468</td>\n",
       "      <td>Organism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2892 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name                               id  category\n",
       "0                abscisic acid                      CHEBI:22152  Molecule\n",
       "1                 norway maple                   NCBITaxon:4025  Organism\n",
       "2                     sycamore                    AUTO:sycamore  Organism\n",
       "3                            -                           AUTO:-      Gene\n",
       "4           acacia holosericea                NCBITaxon:1120455  Organism\n",
       "...                        ...                              ...       ...\n",
       "2887                      gene                        AUTO:gene      Gene\n",
       "2888                    plants                      AUTO:plants      Gene\n",
       "2889                  brassica                   NCBITaxon:3705  Organism\n",
       "2890  plant/algal macrofossils  AUTO:plant/algal%20macrofossils  Molecule\n",
       "2891            quercus cerris                  NCBITaxon:39468  Organism\n",
       "\n",
       "[2892 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_groundings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_auto_dups(grounding_df):\n",
    "    \"\"\"\n",
    "    Remove rows with AUTO annotations if there is another row of the same entity with a better grounding.\n",
    "    \"\"\"\n",
    "    counted = grounding_df.groupby(by='name').count()\n",
    "    duped = counted[counted['id'] >= 2].index.tolist()\n",
    "    print(f'\\nThere are {len(duped)} entities with multiple groundings.')\n",
    "    to_keep = []\n",
    "    for dup in duped:\n",
    "        grounds = grounding_df[grounding_df['name'] == dup].id.values.tolist()\n",
    "        prefixes = [g.split(':')[0] for g in grounds]\n",
    "        if 'AUTO' in prefixes:\n",
    "            # If there's only AUTO, just keep one, the difference is capitalization so it doesn't matter which\n",
    "            if len(set(prefixes)) == 1:\n",
    "                ground_to_keep = grounds[0]\n",
    "            # If there are non-AUTO groundings, check if there's more than one, if so, report and pick randomly for now\n",
    "            if len(set(prefixes)) > 1:\n",
    "                potential_grounds = [g for g in grounds if g.split(':')[0] != 'AUTO']\n",
    "                if len(set(potential_grounds)) == 1:\n",
    "                    ground_to_keep = potential_grounds[0]\n",
    "                else:\n",
    "                    print(f'There is more than one true grounding for entity {dup}:')\n",
    "                    print(potential_grounds)\n",
    "                    ground_to_keep = potential_grounds[0]\n",
    "                    print(f'Choosing first option for now: {ground_to_keep}')\n",
    "        # Get the category corresponding to the correct grounding\n",
    "        try:\n",
    "            cat = grounding_df.loc[(grounding_df['name'] == dup) & (grounding_df['id'] == ground_to_keep), 'category'].values[0]\n",
    "        except IndexError:\n",
    "            cat = 'UNKNOWN'\n",
    "        # Make dict for new row\n",
    "        to_keep.append({'name': dup, 'id': ground_to_keep, 'category': cat})\n",
    "    \n",
    "    # Drop rows with dups, then add back the groundings we keep\n",
    "    grounding_df = grounding_df[~grounding_df['name'].isin(duped)]\n",
    "    grounding_df = pd.concat([grounding_df, pd.DataFrame(to_keep)])\n",
    "    \n",
    "    return grounding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 182 entities with multiple groundings.\n",
      "There is more than one true grounding for entity na:\n",
      "['PR:A3DRP3', 'CHEBI:26708']\n",
      "Choosing first option for now: PR:A3DRP3\n",
      "\n",
      "There are 150 entities with multiple groundings.\n",
      "There is more than one true grounding for entity acc:\n",
      "['CHEBI:18053', 'PR:000022034']\n",
      "Choosing first option for now: CHEBI:18053\n"
     ]
    }
   ],
   "source": [
    "full_grounds = remove_auto_dups(full_groundings)\n",
    "slim_grounds = remove_auto_dups(slim_groundings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For slim OntoGPT:\n",
      "There are 2781 unique entities.\n",
      "Number of entities grounded to each ontology:\n",
      "   CHEBI : 348\n",
      "   NCBITaxon : 286\n",
      "   AUTO : 2031\n",
      "   GO : 24\n",
      "   PR : 92\n",
      "\n",
      "\n",
      "For full OntoGPT:\n",
      "There are 2700 unique entities.\n",
      "Number of entities grounded to each ontology:\n",
      "   CHEBI : 325\n",
      "   NCBITaxon : 503\n",
      "   AUTO : 1762\n",
      "   GO : 28\n",
      "   PR : 82\n"
     ]
    }
   ],
   "source": [
    "for ty, gr in {'slim': slim_grounds, 'full': full_grounds}.items():\n",
    "    print(f'\\n\\nFor {ty} OntoGPT:')\n",
    "    print(f'There are {len(gr)} unique entities.')\n",
    "    labs = gr.id.values.tolist()\n",
    "    counted_prefixes = Counter([i.split(':')[0] for i in labs])\n",
    "    print('Number of entities grounded to each ontology:')\n",
    "    for ont, num in counted_prefixes.items():\n",
    "        print('  ', ont, ':', num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's looking like changing over to the slim NCBI Taxonomy resulted in a loss of almost half of the species groundings. Let's take a more detailed look. First, let's see what the overlap is between the two sets of NCBI annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_ncbi = slim_grounds[slim_grounds['id'].str.contains('NCBITaxon')]\n",
    "full_ncbi = full_grounds[full_grounds['id'].str.contains('NCBITaxon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 214 entities in common between the NCBI-grounded entities of the slim and full OntoGPT runs.\n"
     ]
    }
   ],
   "source": [
    "in_common = set(slim_ncbi['name']).intersection(set(full_ncbi['name']))\n",
    "print(f'There are {len(in_common)} entities in common between the NCBI-grounded entities of the slim and full OntoGPT runs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there are 72 entities that got a grounding with the slim ontology that didn't get one with the full ontology. However, we don't know that the remaining 217 entities were even identified. So let's see if they're in the `AUTO` category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ncbi_names = full_ncbi['name'].values.tolist()\n",
    "identified_in_slim = slim_grounds[slim_grounds['name'].isin(full_ncbi_names)].copy()\n",
    "identified_in_slim['prefix'] = identified_in_slim['id'].str.split(':').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392 of the NCBITaxon-gronded names from the full ontology extraction exist in the slim extraction.\n",
      "Of these, 176 are AUTO-grounded.\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(identified_in_slim)} of the NCBITaxon-gronded names from the full ontology extraction exist in the slim extraction.')\n",
    "counted_cats = identified_in_slim.groupby(by='prefix').count()\n",
    "print(f'Of these, {counted_cats.loc[\"AUTO\", \"id\"]} are AUTO-grounded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that almost 100 entities weren't even identified; let's see what they were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_in_slim = full_ncbi[~full_ncbi['name'].isin(identified_in_slim.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macrobiotus hufelandi',\n",
       " 'green gram',\n",
       " 'black gram',\n",
       " 'sesamum',\n",
       " 'anna (lowland)',\n",
       " 'cassava',\n",
       " 'heterorhabditis georgiana',\n",
       " 'acheta domesticus',\n",
       " 'agrotis ipsilon',\n",
       " 'diaprepes abbreviatus',\n",
       " 'musca domestica',\n",
       " 'plodia interpunctella',\n",
       " 'solenopsis invicta',\n",
       " 'tenebrio molitor',\n",
       " 'heterorhabditis floridensis',\n",
       " 'heterorhabditis indica',\n",
       " 'heterorhabditis mexicana',\n",
       " 'steinernema carpocapsae',\n",
       " 'steinernema feltiae',\n",
       " 'steinernema rarum']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_in_slim.name.values.tolist()[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ontogpt",
   "language": "python",
   "name": "ontogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
