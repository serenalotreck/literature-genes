{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple link prediction\n",
    "In this notebook, we'll implement a simple RF-based link prediction method where we'll train on a previous year to predict the subsequent year. In past iterations, we have trained on all years prior to 2020 and then predicted on all years past 2020, but this appeared to cause data leakage in a way we didn't understand. So for the sake of illustration, here I'll just take the year with the most data and predict the subsequent year, to see if this clears upt he data leakage issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import median\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_graphml('../data/kg/all_drought_dt_co_occurrence_graph_02May2024.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of ML problem\n",
    "We want to predict desiccation edges that appear in the future of the graph. In order to prevent data leakage, we'll formulate this problem as predicting just edges that appear in the next year. We can then string together sets of predictions to perform forecasting farther into the future.\n",
    "\n",
    "Starting in 2015, we'll slice the graph for each year. The training set features will be built from the year with the most new links, and the training labels will be taken from the subsequent year. The test set features will be built from the same training features year, but the links taken from the year following the training links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "In a past notebook (now removed from this repo, but still in thie history) we explored maunal feature extraction, so I will just jump straight to extracting them with the code from that notebook. We do, however, need to adjust this code to only use data up to one year for feature extraction for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_features(pair_names, edges, per_node_features):\n",
    "    \"\"\"\n",
    "    Get node pair features.\n",
    "    \"\"\"\n",
    "    all_paired_features = {}\n",
    "    for n1, n2 in pair_names:\n",
    "        pair_features = {}\n",
    "        # Check if it's an edge\n",
    "        if ((n1, n2) in edges.keys()) or ((n2, n1) in edges.keys()):\n",
    "            is_edge = True\n",
    "            try:\n",
    "                edge_attrs = edges[(n1, n2)]\n",
    "                key_order = (n1, n2)\n",
    "            except KeyError:\n",
    "                edge_attrs = edges[(n2, n1)]\n",
    "                key_order = (n2, n1)\n",
    "        else:\n",
    "            is_edge = False\n",
    "            key_order = (n1, n2)\n",
    "        # Merge the attributes for the individual nodes\n",
    "        for k, v in per_node_features[n1].items():\n",
    "            pair_features[f'{k}_n1'] = v\n",
    "        for k, v in per_node_features[n2].items():\n",
    "            pair_features[f'{k}_n2'] = v\n",
    "        # Add composite features/labels\n",
    "        if is_edge:\n",
    "            pair_features['are_connected'] = True\n",
    "            if edge_attrs['is_drought']:\n",
    "                pair_features['is_drought_edge'] = True\n",
    "                if edge_attrs['is_desiccation']:\n",
    "                    pair_features['is_desiccation_edge'] = True\n",
    "                else:\n",
    "                    pair_features['is_desiccation_edge'] = False\n",
    "            else:\n",
    "                pair_features['is_drought_edge'] = False\n",
    "                if edge_attrs['is_desiccation']:\n",
    "                    pair_features['is_desiccation_edge'] = True\n",
    "                else:\n",
    "                    pair_features['is_desiccation_edge'] = False\n",
    "            pair_features['year_first_connected'] = edge_attrs['first_year_mentioned']\n",
    "        else:\n",
    "            pair_features['are_connected'] = False\n",
    "            pair_features['is_drought_edge'] = False\n",
    "            pair_features['is_desiccation_edge'] = False\n",
    "        all_paired_features[key_order] = pair_features\n",
    "        \n",
    "    return all_paired_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(graph, cut_years):\n",
    "    \"\"\"\n",
    "    Get the per-node features for each year's instances.\n",
    "    \"\"\"\n",
    "    year_per_node_features = {}\n",
    "    year_graphs = {}\n",
    "    for cut_year in cut_years:\n",
    "        # Get rid of anything after the cut year in the graph\n",
    "        pre_cutoff_graph = nx.Graph()\n",
    "        new_nodes = [(n, attrs) for n, attrs in graph.nodes(data=True)\n",
    "                               if int(attrs['first_year_mentioned']) < cut_year]\n",
    "        new_edges = [(e1, e2, attrs) for e1, e2, attrs in graph.edges(data=True)\n",
    "                               if int(attrs['first_year_mentioned']) < cut_year]\n",
    "        _ = pre_cutoff_graph.add_nodes_from(new_nodes)\n",
    "        _ = pre_cutoff_graph.add_edges_from(new_edges)\n",
    "        year_graphs[cut_year] = pre_cutoff_graph\n",
    "    \n",
    "        # Get the features for each node\n",
    "        per_node_features = defaultdict(dict)\n",
    "\n",
    "        # Degree\n",
    "        g_degree = pre_cutoff_graph.degree()\n",
    "        for n, deg in g_degree:\n",
    "            per_node_features[n]['degree'] = deg\n",
    "\n",
    "        # Entity type, year, number of node mentions\n",
    "        for n, attrs in pre_cutoff_graph.nodes(data=True):\n",
    "            per_node_features[n]['ent_type'] = attrs['ent_type']\n",
    "            per_node_features[n]['node_year_first_mentioned'] = attrs['first_year_mentioned']\n",
    "\n",
    "        # Edge-based feature basics\n",
    "        years_added = defaultdict(list)\n",
    "        node_des_only_edge_counts = defaultdict(int)\n",
    "        for e1, e2, attrs in tqdm(pre_cutoff_graph.edges(data=True)):\n",
    "            # Years\n",
    "            years_added[e1].append(int(attrs['first_year_mentioned']))\n",
    "            years_added[e2].append(int(attrs['first_year_mentioned']))\n",
    "            # Counts for edge proportion\n",
    "            if attrs['is_desiccation']:\n",
    "                if not attrs['is_drought']:\n",
    "                    node_des_only_edge_counts[e1] += 1\n",
    "                    node_des_only_edge_counts[e2] += 1\n",
    "        \n",
    "        # Process edge based features\n",
    "        for n in pre_cutoff_graph.nodes:\n",
    "            # Years\n",
    "            try:\n",
    "                years = years_added[n]\n",
    "                counted = Counter(years)\n",
    "                per_node_features[n]['avg_year_additions'] = sum(counted.values())/len(counted)\n",
    "                per_node_features[n]['median_year_added'] = median(years)\n",
    "            except ZeroDivisionError:\n",
    "                per_node_features[n]['avg_year_additions'] = 0\n",
    "                per_node_features[n]['median_year_added'] = 0\n",
    "            # Edge proportion\n",
    "            try:\n",
    "                des_count = node_des_only_edge_counts[n]\n",
    "                per_node_features[n]['des_only_prop'] = des_count/g_degree[n]\n",
    "            except ZeroDivisionError:\n",
    "                per_node_features[n]['des_only_prop'] = 0\n",
    "        year_per_node_features[cut_year] = per_node_features\n",
    "        \n",
    "    return year_per_node_features, year_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_and_negative_pairs(graph, cut_years):\n",
    "    \"\"\"\n",
    "    Get the pair names for positive and negative instances. Negative instances are negative over all time (for which\n",
    "    the graph contains data), but are balanced for each year and only contain feature information for the same time\n",
    "    period as the positive instances for that year.\n",
    "    \"\"\"\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    all_negative_names = []\n",
    "    node_years = nx.get_node_attributes(graph, 'first_year_mentioned')\n",
    "    for cut_year in cut_years:\n",
    "        \n",
    "        # Get positive instances, which are edges added in the cut year\n",
    "        positive_pair_names = [(e1, e2) for e1, e2, attrs in graph.edges(data=True)\n",
    "                               if attrs['is_desiccation'] and (int(attrs['first_year_mentioned']) == cut_year)]\n",
    "        positive_pair_names = [pair for pair in positive_pair_names if (int(node_years[pair[0]]) < cut_year)\n",
    "                              and (int(node_years[pair[1]]) < cut_year)]\n",
    "        print(f'There are {len(positive_pair_names)} positive pairs for year {cut_year}.')\n",
    "        \n",
    "        # Get negative instances, which are pairs of nodes that never get an edge\n",
    "        negative_pair_candidates = combinations(list(graph.nodes), 2)\n",
    "        negative_pair_names = []\n",
    "        for pair in negative_pair_candidates:\n",
    "            if len(negative_pair_names) == len(positive_pair_names):\n",
    "                break\n",
    "            else:\n",
    "                if not graph.has_edge(pair[0], pair[1]): # If they're never connected\n",
    "                    if pair not in all_negative_names: # And it's not already a negative\n",
    "                        if (int(node_years[pair[0]]) < cut_year) and (int(node_years[pair[1]]) < cut_year): # And the nodes exist prior to this year\n",
    "                            negative_pair_names.append(pair) # Add it as a negative\n",
    "                            all_negative_names.append(pair)\n",
    "        print(f'A corresponding {len(negative_pair_names)} negative instances have been constructed for year {cut_year}.')\n",
    "    \n",
    "        # Add to the dictionaries for the year\n",
    "        positives[cut_year] = positive_pair_names\n",
    "        negatives[cut_year] = negative_pair_names\n",
    "        \n",
    "    return positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_table(graph, cut_years=[2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], test_cutoff=2020):\n",
    "    \"\"\"\n",
    "    Build the feature table. Current features are:\n",
    "        Node degree\n",
    "        Entity type\n",
    "        Proportion of edges for each node that are desiccation or drought\n",
    "        Rate of edge addition for each node (using first year of mention)\n",
    "        Whether or not the other two nodes are already connected by a drought edge\n",
    "    \"\"\"\n",
    "    # Get the pair names for positive and negative instances from full graph, using cutoff\n",
    "    print('\\nGetting positive and negative instances...')\n",
    "    positives, negatives = get_positive_and_negative_pairs(graph, cut_years)\n",
    "    \n",
    "    # Build single-node feature vectors\n",
    "    print('\\nWrangling features for each node...')\n",
    "    year_per_node_features, year_graphs = get_node_features(graph, cut_years)\n",
    "\n",
    "    # Build paird feature vectors\n",
    "    year_paired_feature_dfs = {}\n",
    "    for cut_year in cut_years:\n",
    "        \n",
    "        # Get edges from this year\n",
    "        edges = {e[0]: e[1] for e in year_graphs[cut_year].edges(data=True)}\n",
    "        \n",
    "        # Get positive paired feature vectors\n",
    "        pos_instances = get_paired_features(positives[cut_year], edges, year_per_node_features[cut_year])\n",
    "        pos_df = pd.DataFrame.from_records(pos_instances).T\n",
    "        pos_df['label'] = 1\n",
    "        pos_df['year'] = cut_year\n",
    "    \n",
    "        # get negative positive paired feature vectors\n",
    "        neg_instances = get_paired_features(negatives[cut_year], edges, year_per_node_features[cut_year])\n",
    "        neg_df = pd.DataFrame.from_records(neg_instances).T\n",
    "        neg_df['label'] = 0\n",
    "        neg_df['year'] = cut_year\n",
    "        \n",
    "        year_paired_feature_dfs[cut_year] = [pos_df, neg_df]\n",
    "\n",
    "    # Make train and test dataframes\n",
    "    print('\\nMaking feature table...')\n",
    "    year_paired_feature_dfs = {cut_year: pd.concat([dfs[0], dfs[1]]) for cut_year, dfs in year_paired_feature_dfs.items()}\n",
    "    train_df = pd.concat([df for cut_year, df in year_paired_feature_dfs.items() if cut_year < test_cutoff])\n",
    "    test_df = pd.concat([df for cut_year, df in year_paired_feature_dfs.items() if cut_year >= test_cutoff])\n",
    "    print(f'The training dataframe has shape {train_df.shape}.')\n",
    "    print(f'The test dataframe has shape {test_df.shape}.')\n",
    "    \n",
    "    print('\\nDone!')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unit_test_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unit_test_train, unit_test_test \u001b[38;5;241m=\u001b[39m build_feature_table(\u001b[43munit_test_graph\u001b[49m, cut_years\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1997\u001b[39m, \u001b[38;5;241m1998\u001b[39m, \u001b[38;5;241m1999\u001b[39m, \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m2001\u001b[39m, \u001b[38;5;241m2002\u001b[39m, \u001b[38;5;241m2003\u001b[39m, \u001b[38;5;241m2004\u001b[39m, \u001b[38;5;241m2005\u001b[39m, \u001b[38;5;241m2006\u001b[39m, \u001b[38;5;241m2007\u001b[39m], test_cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2005\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unit_test_graph' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
